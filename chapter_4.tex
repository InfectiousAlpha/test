\ifdefined\ispartofbook
\else
  \input{preamble.tex}
  \begin{document}
\fi

\chapter{Theoretical Analysis of Network Internals}
\label{chap:theory}

This chapter will transition from the mechanics of \textit{how} networks learn to the advanced theory of \textit{why} they behave the way they do. It will serve as a deep dive into the internal dynamics and statistical properties of neural networks, providing the theoretical tools necessary to analyze and understand phenomena that are not apparent from the basic learning algorithm alone. We will begin by exploring the fundamental consequences of the rectification operation, analyzing the "inverse problem" of inferring a signal's original properties after it has been transformed. This analysis will start with a simple one-dimensional case and progressively build in complexity to the full N-dimensional scenario, revealing the profound challenges of information loss and the "curse of dimensionality." The second half of the chapter will shift focus to the dynamics of information flow, using the framework of Analytical Moment Propagation to understand how signals and errors propagate through the network, providing a first-principles explanation for everything from weight initialization to the very nature of gradient noise.

\section{The 1D Inverse Problem: A Foundation in Information Loss}
\label{sec:inverse_1d}

The "inverse problem" for the rectified Gaussian distribution is a foundational challenge in statistical inference that serves as a perfect microcosm for understanding the consequences of non-linear transformations in neural networks. It involves a simple question: if we observe the statistical properties of a signal \textit{after} it has passed through a ReLU activation, what can we infer about the signal's properties \textit{before} the activation? The answer reveals a fundamental and irreversible loss of information that has deep implications.

\subsection{Formulating the Forward and Inverse Problems}
The analysis begins with the forward problem. We assume a normally distributed input variable, $X \sim \mathcal{N}(\mu, \sigma^2)$, which represents the pre-activation of a single neuron. This variable is passed through the rectification operation, yielding an output $Y = \max(0, X)$. As established in Chapter \ref{chap:relu}, the output $Y$ follows a rectified Gaussian distribution, and its first two moments (mean $\mu_Y$ and variance $\sigma^2_Y$) can be calculated analytically from the input parameters $(\mu, \sigma)$.

The inverse problem reverses this question: given an empirically observed mean $\mu_{Y, \text{obs}}$ and variance $\sigma^2_{Y, \text{obs}}$, can we uniquely determine the original parameters $(\mu, \sigma)$? This is equivalent to solving the following system of two coupled, non-linear, transcendental equations:
\begin{align}
    \mu_Y(\mu, \sigma) - \mu_{Y, \text{obs}} &= 0 \\
    \sigma^2_Y(\mu, \sigma) - \sigma^2_{Y, \text{obs}} &= 0
\end{align}

\subsection{The Ill-Posed Nature of the Problem}
An inverse problem is considered ill-posed if it fails to satisfy one or more of Hadamard's conditions: existence, uniqueness, and stability of the solution. The inverse rectified Gaussian problem can fail on all three counts, primarily due to \textbf{information loss}. The rectification operation, $Y = \max(0, X)$, is a \textbf{many-to-one mapping}. It collapses the entire negative portion of the original Gaussian's domain, the interval $(-\infty, 0]$, onto the single point $y=0$. This process irreversibly erases all information regarding the shape, scale, and moments of the negative tail of the original distribution.

This information loss leads to distinct solution regimes:
\begin{itemize}
    \item \textbf{Unique Solution:} For most pairs of observed moments $(\mu_Y, \sigma^2_Y)$ that could plausibly have been generated by this process, a single, unique solution for $(\mu, \sigma)$ exists. This "achievable region" of moments defines the space where the inverse problem is well-posed.
    \item \textbf{Infinite Solutions (Non-Uniqueness):} A pathological case of non-uniqueness occurs at the boundary of this region. Specifically, if we observe $\mu_Y = 0$ and $\sigma^2_Y = 0$, it is impossible to distinguish the source. This outcome could have been generated by any underlying Gaussian distribution with a non-positive mean ($\mu \le 0$) and a variance approaching zero ($\sigma \to 0$).
    \item \textbf{No Solution (Non-Existence):} If an empirically measured pair of moments falls outside the achievable region, no real-valued solution for $(\mu, \sigma)$ exists. This serves as a powerful model diagnostic, providing strong statistical evidence that the data-generating process was not a simple rectified Gaussian.
\end{itemize}

\subsection{Dual Origins: Econometrics and Machine Learning}
This exact mathematical problem has a rich history in two disparate fields, highlighting its fundamental importance.
\begin{itemize}
    \item \textbf{Econometrics:} In econometrics, the problem is central to the \textbf{Tobit model}, proposed by James Tobin in 1958 \cite{Tobin1958LimitedDependent}. The Tobit model is used to analyze "censored" data, such as household expenditure on a particular good, where the expenditure cannot be negative and a significant number of households report zero expenditure. Estimating the underlying factors that drive the latent (unconstrained) desire to spend requires solving this inverse problem.
    \item \textbf{Machine Learning:} With the rise of deep learning, the problem gained new prominence in the analysis of networks with \textbf{Rectified Linear Unit (ReLU)} activation functions \cite{Socci1998RectifiedGaussian}. Understanding the statistical properties of a neuron's output requires solving the forward problem, and inferring the properties of its input requires tackling the inverse problem.
\end{itemize}

\section{The 2D Inverse Problem: Inferring Latent Statistics of Correlated Neurons}
\label{sec:inverse_2d}

The extension of the inverse problem to two dimensions introduces a significant escalation in complexity. This is not merely a matter of increasing the number of equations; the introduction of a correlation parameter fundamentally alters the structure of the problem and the nature of the information loss. The 2D case models the joint statistics of two correlated ReLU neurons.

\subsection{Amplification of Complexity}
The underlying process now begins with a vector of two random variables, $\mathbf{X} = (X_1, X_2)$, drawn from a bivariate normal distribution characterized by five parameters: two means ($\mu_1, \mu_2$), two standard deviations ($\sigma_1, \sigma_2$), and a correlation coefficient ($\rho$). The rectification is applied element-wise, $\mathbf{Y} = (\max(0, X_1), \max(0, X_2))$. The inverse problem is to recover the five original parameters from the five rectified moments (two means, two variances, and one covariance).

The primary driver of the amplified complexity is the correlation parameter, $\rho$. In the 2D problem, the rectifications of $X_1$ and $X_2$ are not independent events; they are statistically coupled. The probability that $X_1$ is negative is conditional on the value of $X_2$, and vice versa. This deep intertwining makes it impossible to separate the resulting system of five non-linear, transcendental equations.

\subsection{Partitioned Information Loss}
The nature of the ill-posedness is also fundamentally altered. In the 1D case, information loss occurs over a single interval. In the 2D case, the rectification collapses three of the four quadrants of the original $(X_1, X_2)$ plane:
\begin{enumerate}
    \item The third quadrant $(X_1 < 0, X_2 < 0)$ is mapped entirely to the origin $(0,0)$.
    \item The second quadrant $(X_1 < 0, X_2 > 0)$ is mapped onto the positive $y_2$-axis.
    \item The fourth quadrant $(X_1 > 0, X_2 < 0)$ is mapped onto the positive $y_1$-axis.
\end{enumerate}
This partitioned information loss creates a much richer set of possibilities for ill-posedness. Different combinations of the original five parameters could potentially yield the same rectified moments if they produce offsetting changes in the contributions from these three quadrants. This suggests that the solution space for the inverse problem may contain complex, multi-dimensional manifolds of non-uniqueness, a far more challenging scenario than the single pathological point found in the 1D case.

\section{The N-Dimensional Inverse Problem and the Curse of Dimensionality}
\label{sec:inverse_nd}

The challenges identified in the 2D case are severely exacerbated as the problem is extended to N dimensions. An N-dimensional rectified Gaussian is defined by $N + N(N+1)/2$ parameters (N means and a symmetric $N \times N$ covariance matrix). The inverse problem requires solving a system of the same number of equations.

\subsection{Combinatorial Explosion and Computational Intractability}
The core of the problem is a combinatorial explosion. The rectification operator partitions the $N$-dimensional space $\mathbb{R}^N$ into $2^N$ orthants. The resulting rectified distribution is a complex mixture of $2^N$ components, with probability mass distributed on the interior and all lower-dimensional boundaries of the non-negative orthant.

The forward problem itself becomes computationally formidable. Calculating the rectified moments requires evaluating integrals of the N-dimensional Gaussian PDF over these orthants. These integrals, known as Truncated Multivariate Normal (TMVN) moments, lack general closed-form solutions for $N > 2$ and suffer from the "curse of dimensionality" \cite{KanRobotti2017TruncatedMoments}. Their computation relies on sophisticated numerical methods like recurrence relations or Monte Carlo integration, which are computationally expensive.

\subsection{The Paradigm Shift to Data-Driven Inversion}
The sheer scale and aggravated ill-posedness of the N-dimensional inverse problem render classical numerical root-finding methods impractical. This computational intractability necessitates a paradigm shift away from direct, model-based inversion towards indirect, data-driven frameworks.

This strongly suggests that for any problem of non-trivial dimensionality, the \textbf{data-driven machine learning surrogate approach} is not merely an alternative but likely the \textit{only} feasible path forward. This involves:
\begin{enumerate}
    \item Generating a large synthetic dataset by repeatedly solving the computationally expensive forward problem.
    \item Training a deep neural network or a conditional generative model (like a Variational Autoencoder) to learn the inverse mapping from rectified moments back to the original parameters.
\end{enumerate}
This approach amortizes the high offline computational cost to enable fast online inference. Crucially, using a generative model can capture the full posterior distribution of possible solutions, elegantly handling the problem's inherent non-uniqueness by treating it as a feature to be modeled rather than a bug to be overcome.

\section{The Dynamics of Deep Networks: Forward Signal Propagation and Expressivity}
\label{sec:forward_dynamics}

A deep feedforward network can be viewed as a dynamical system, where the layer index acts as a discrete time variable. The stability of this system is paramount for trainability. \textbf{Mean Field Theory (MFT)}, borrowed from statistical physics, provides a powerful lens to analyze this stability in infinitely wide networks \cite{Schoenholz2017DeepInfoProp}.

MFT predicts a sharp phase transition between two regimes:
\begin{itemize}
    \item An \textbf{ordered phase}, where input signals quickly converge to a single fixed point. The network loses all distinguishing information, corresponding to the problem of \textbf{vanishing gradients}.
    \item A \textbf{chaotic phase}, where initially similar inputs diverge exponentially. This leads to unstable and unpredictable behavior, corresponding to \textbf{exploding gradients}.
\end{itemize}
The boundary between these two regimes is known as the \textbf{"edge of chaos."} It is at this critical point that a network can propagate signals through many layers while preserving information, a condition known as \textit{dynamical isometry}. Networks initialized at the edge of chaos are found to be the most trainable \cite{Poole2016ExponentialExpressivity}.

Principled weight initialization schemes are the practical application of this theory, designed to place a network at this critical point.
\begin{itemize}
    \item \textbf{Xavier/Glorot Initialization:} Derived to preserve signal variance in networks with symmetric activations like tanh, it sets the weight variance to $\text{Var}(W) = 2/(n_{in} + n_{out})$ \cite{GlorotBengio2010Difficulty}.
    \item \textbf{He/Kaiming Initialization:} Derived specifically for ReLU, it accounts for the halving of variance caused by the rectification operation and sets the weight variance to $\text{Var}(W) = 2/n_{in}$ \cite{He2015Delving}.
\end{itemize}
These initialization rules are not heuristics; they are direct consequences of the requirement for stable signal propagation as described by the mathematics of moment propagation.

\section{The Dynamics of Learning: Backward Propagation of Error Statistics}
\label{sec:backward_dynamics}

The framework of Analytical Moment Propagation (AMP) can be extended from the forward pass to the backward pass, providing a theoretical basis for understanding the statistics of the signals that drive learning: the error signals and the gradients.

\subsection{A Statistical View of Backpropagation}
To analyze its statistical properties, we reframe backpropagation as a stochastic process. The error signal at layer $l$, $\boldsymbol{\delta}_l$, and the gradients, $\nabla\mathbf{W}_l$ and $\nabla\mathbf{b}_l$, are treated as random variables whose distributions are shaped by the statistics of the input data. The goal of backward AMP is to derive the analytical expressions for their mean and covariance.

\subsection{The Entanglement of Forward and Backward Passes}
A crucial insight from this analysis is the deep statistical entanglement between the forward and backward passes. The error signal at layer $l$ is computed as $\boldsymbol{\delta}^{[l]} = ((\vect{W}^{[l+1]})^T \boldsymbol{\delta}^{[l+1]}) \odot g'^{[l]}(\vect{Z}^{[l]})$. The term $g'^{[l]}(\vect{Z}^{[l]})$ depends on the forward-pass pre-activations, creating a multiplicative coupling.

This means that a complete description requires tracking not only the moments of the forward and backward signals independently, but also their \textbf{cross-covariance}. The mean of the weight gradient, for example, is not simply the product of the mean activation and mean error; it includes a crucial correction term that depends on the cross-covariance between the forward-propagating signal and the backward-propagating error.
\begin{equation}
E[\text{vec}(\nabla\mathbf{W}_l)] = \boldsymbol{\mu}_{h,l-1} \otimes \boldsymbol{\mu}_{\delta,l} + \text{vec}(\text{Cov}(\boldsymbol{\delta}_l, \mathbf{h}_{l-1}))
\end{equation}

\subsection{Implications for Gradient Noise and Optimization}
This framework provides a first-principles characterization of "gradient noise" in stochastic gradient descent. The derivation of the full gradient covariance matrix, $\text{Cov}(\text{vec}(\nabla\mathbf{W}_l))$, reveals that this noise is highly structured and anisotropic, not simple isotropic noise as is often assumed for convenience. Its covariance depends on the input data statistics, the network architecture, and the current values of the parameters. This provides a theoretical foundation for understanding the behavior of adaptive optimizers like Adam \cite{KingmaBa2014Adam} and offers a more rigorous, second-order statistical view on the classic vanishing and exploding gradient problems.

\ifdefined\ispartofbook
\else
  \input{references.tex}
  \end{document}
\fi

\begin{thebibliography}{99}

\bibitem{AdadiBerrada2018XAISurvey}
Adadi, A., \& Berrada, M. (2018). Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). \textit{IEEE Access}.

\bibitem{AlainBengio2016Probes}
Alain, G., \& Bengio, Y. (2016). Understanding intermediate layers using linear classifier probes. \textit{arXiv preprint arXiv:1610.01644}.

\bibitem{Amemiya1985Econometrics}
Amemiya, T. (1985). \textit{Advanced Econometrics}. Harvard University Press.

\bibitem{Arora2025ReLUOutputDist}
Arora, R., Basu, A., Mianjy, P., \& Mukherjee, A. (2025). On the Exact Computation of the Output Distribution of a ReLU Network. \textit{arXiv preprint arXiv:2503.22082}.

\bibitem{BarrSherrill1999TruncatedNormal}
Barr, D. R., \& Sherrill, E. T. (1999). Mean and variance of truncated normal distributions. \textit{The American Statistician, 53}(4), 357-361.

\bibitem{Bishop2006PatternRecognition}
Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.

\bibitem{Bottou2018Optimization}
Bottou, L., Curtis, F. E., \& Nocedal, J. (2018). Optimization methods for large-scale machine learning. \textit{SIAM Review}, 60(2), 223-311.

\bibitem{ChoSaul2014RectifiedGaussian}
Cho, K., \& Saul, L. K. (2014). On the properties of the rectified Gaussian distribution. \textit{arXiv preprint arXiv:1406.4533}.

\bibitem{DansbeckerRELUKaggle}
Dansbecker, M. (n.d.). \textit{Rectified Linear Units (ReLU) in Deep Learning}. Kaggle. Retrieved July 23, 2025, from \url{https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning}

\bibitem{Ding2013RectifiedFactor}
Ding, Z., He, Z., \& Carin, L. (2013). Rectified factor networks. In \textit{Advances in Neural Information Processing Systems 26} (pp. 1106-1114).

\bibitem{FreyHinton1999Transformations}
Frey, B. J., \& Hinton, G. E. (1999). Variational learning in nonlinear Gaussian belief networks. \textit{Neural Computation}, 11(1), 193-213.

\bibitem{Fukushima1980Neocognitron}
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. \textit{Biological Cybernetics}, 36(4), 193-202.

\bibitem{GalGhahramani2016DropoutBayes}
Gal, Y., \& Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In \textit{International conference on machine learning} (pp. 1050-1059).

\bibitem{Genz2009Computation}
Genz, A., \& Bretz, F. (2009). \textit{Computation of multivariate normal and t probabilities}. Springer Science \& Business Media.

\bibitem{Geweke1991Gibbs}
Geweke, J. (1991). Efficient simulation from the multivariate normal and Student-t distributions subject to linear constraints. In \textit{Computing Science and Statistics: Proceedings of the 23rd Symposium on the Interface} (pp. 571-578).

\bibitem{GlorotBordesBengio2011DeepSparse}
Glorot, X., Bordes, A., \& Bengio, Y. (2011). Deep sparse rectifier neural networks. In \textit{Proceedings of the fourteenth international conference on artificial intelligence and statistics} (pp. 315-323).

\bibitem{GlorotBengio2010Difficulty}
Glorot, X., \& Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In \textit{Proceedings of the thirteenth international conference on artificial intelligence and statistics} (pp. 249-256).

\bibitem{Goodfellow2016Book}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.

\bibitem{HaninRolnick2019ActivationPatterns}
Hanin, B., \& Rolnick, D. (2019). Deep ReLU Networks Have Surprisingly Few Activation Patterns. \textit{Advances in Neural Information Processing Systems}.

\bibitem{He2015Delving}
He, K., Zhang, X., Ren, S., \& Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In \textit{Proceedings of the IEEE international conference on computer vision} (pp. 1026-1034).

\bibitem{Hochreiter2001GradientFlow}
Hochreiter, S., Bengio, Y., Frasconi, P., \& Schmidhuber, J. (2001). \textit{Gradient flow in recurrent nets: the difficulty of learning long-term dependencies}. In A field guide to dynamical recurrent networks. IEEE Press.

\bibitem{Hornik1989UniversalApprox}
Hornik, K., Stinchcombe, M., \& White, H. (1989). \textit{Multilayer feedforward networks are universal approximators}. Neural Networks, 2(5), 359-366.

\bibitem{KanRobotti2017TruncatedMoments}
Kan, R., \& Robotti, C. (2017). On the moments of folded and truncated multivariate normal distributions. \textit{Journal of Computational and Graphical Statistics, 26}(4), 930-934.

\bibitem{KingmaBa2014Adam}
Kingma, D. P., \& Ba, J. (2014). Adam: A method for stochastic optimization. \textit{arXiv preprint arXiv:1412.6980}.

\bibitem{LeCun1998MNIST}
LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998). Gradient-based learning applied to document recognition. \textit{Proceedings of the IEEE}, 86(11), 2278-2324.

\bibitem{LeCun2012EfficientBackprop}
LeCun, Y., Bottou, L., Orr, G. B., \& Müller, K. R. (2012). Efficient backprop. In \textit{Neural networks: Tricks of the trade} (pp. 9-48). Springer.

\bibitem{Liu2022RectifiedFlow}
Liu, X., Gong, C., \& Liu, Q. (2022). Rectified flow: A marginal preserving approach to optimal transport. \textit{arXiv preprint arXiv:2209.14577}.

\bibitem{LundbergLee2017SHAP}
Lundberg, S. M., \& Lee, S. I. (2017). A unified approach to interpreting model predictions. In \textit{Advances in neural information processing systems} (pp. 4765-4774).

\bibitem{Mazur2015BackpropExample}
Mazur, M. (2015). A Step by Step Backpropagation Example. \textit{Matt Mazur blog}. Retrieved August 7, 2025, from \url{https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/}

\bibitem{Murphy2012ML}
Murphy, K. P. (2012). \textit{Machine Learning: A Probabilistic Perspective}. MIT Press.

\bibitem{Muthen1990CensoredMoments}
Muthén, B. (1990). \textit{Moments of the censored and truncated bivariate normal distribution}. British Journal of Mathematical and Statistical Psychology, 43(1), 131-143.

\bibitem{NairHinton2010ReLU}
Nair, V., \& Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In \textit{Proceedings of the 27th international conference on machine learning (ICML-10)} (pp. 807-814).

\bibitem{Nielsen2015Book}
Nielsen, M. A. (2015). \textit{Neural networks and deep learning}. Determination press.

\bibitem{Pinkus1999ApproximationTheory}
Pinkus, A. (1999). \textit{Approximation theory of the MLP model in neural networks}. Acta Numerica, 8, 143-195.

\bibitem{Poole2016ExponentialExpressivity}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., \& Ganguli, S. (2016). Exponential expressivity in deep neural networks through transient chaos. \textit{Advances in neural information processing systems}, 29.

\bibitem{Ribeiro2016LIME}
Ribeiro, M. T., Singh, S., \& Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}.

\bibitem{Rudin2019StopExplaining}
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. \textit{Nature Machine Intelligence}, 1(5), 206-215.

\bibitem{Rumelhart1986Backprop}
Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). Learning representations by back-propagating errors. \textit{Nature}, 323(6088), 533-536.

\bibitem{Schiendorfer2020BackpropExample}
Schiendorfer, A. (2020). A worked example of backpropagation. \textit{Connecting deep dots blog}. Retrieved August 7, 2025, from \url{https://alexander-schiendorfer.github.io/2020/02/24/a-worked-example-of-backprop.html}

\bibitem{Schoenholz2017DeepInfoProp}
Schoenholz, S. S., Gilmer, J., Ganguli, S., \& Sohl-Dickstein, J. (2017). Deep information propagation. \textit{arXiv preprint arXiv:1611.01232}.

\bibitem{Selvaraju2017GradCAM}
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., \& Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In \textit{Proceedings of the IEEE international conference on computer vision} (pp. 618-626).

\bibitem{Socci1998RectifiedGaussian}
Socci, N. D., Lee, D. D., \& Seung, H. S. (1998). The rectified gaussian distribution. In \textit{Advances in Neural Information Processing Systems} (pp. 350-356).

\bibitem{Tallis1961MGF}
Tallis, G. M. (1961). The moment generating function of the truncated multi-normal distribution. \textit{Journal of the Royal Statistical Society: Series B (Methodological)}, 23(1), 223-229.

\bibitem{Tobin1958LimitedDependent}
Tobin, J. (1958). \textit{Estimation of Relationships for Limited Dependent Variables}. Econometrica, 26(1), 24-36.

\bibitem{Wright2024AnalyticCovariance}
Wright, L., et al. (2024). An Analytic Solution to Covariance Propagation in Neural Networks. \textit{Proceedings of Machine Learning Research}.

\bibitem{Zayyani2016FastRectified}
Zayyani, H., Babaie-Zadeh, M., \& Jutten, C. (2016). Fast rectified sparse Bayesian learning. \textit{IEEE Transactions on Signal Processing}, 64(14), 3788-3799.

\end{thebibliography}
